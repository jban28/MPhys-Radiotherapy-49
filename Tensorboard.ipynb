{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorboard.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jban28/MPhys-Radiotherapy-49/blob/main/Tensorboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E8F_po0K_Wu"
      },
      "source": [
        "## Pre-requisites\n",
        "This block makes the necessary installations and imports for the rest of the code blocks to run, connects to the GPU if one is available, and specifies the location of the folder containing the data. That data folder should contain a sub-folder containing all nifti files, along with a metadata csv file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5AFQEgZc7XUV",
        "outputId": "717de71d-1c11-4f86-9fc2-26536dbd83e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Egh9uSI77b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22eaf270-fdda-4027-a6ee-59e694252e59"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install opencv-contrib-python\n",
        "!pip install scikit-learn\n",
        "!pip install SimpleITK\n",
        "!pip install kornia\n",
        "!pip install utils\n",
        "!pip install torchio\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import SimpleITK as sitk\n",
        "import torch\n",
        "import torchio as tio\n",
        "import kornia.augmentation as K\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from torch.nn import Module\n",
        "from torch.nn import Conv3d\n",
        "from torch.nn import Linear\n",
        "from torch.nn import MaxPool2d\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import LogSoftmax\n",
        "from torch.nn import LeakyReLU\n",
        "from torch import flatten\n",
        "from torch import nn\n",
        "from torch import reshape\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.optim import Adam\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torchvision.io import read_image\n",
        "from torchsummary import summary\n",
        "from scipy.ndimage import zoom, rotate\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "#from torch.utils.data import windowLevelNormalize\n",
        "\n",
        "#set tag\n",
        "tag = 0\n",
        "\n",
        "# Connect to GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using {device} device')\n",
        "\n",
        "# Specify project folder location\n",
        "#project_folder = \"/content/drive/My Drive/Degree/MPhys/Data/\"\n",
        "project_folder = \"/content/drive/My Drive/Data/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.7/dist-packages (2.1.1)\n",
            "Requirement already satisfied: kornia in /usr/local/lib/python3.7/dist-packages (0.6.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (3.0.7)\n",
            "Requirement already satisfied: utils in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: torchio in /usr/local/lib/python3.7/dist-packages (0.18.74)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torchio) (1.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from torchio) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from torchio) (1.21.5)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from torchio) (1.10.0+cu111)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (from torchio) (0.5.1)\n",
            "Requirement already satisfied: SimpleITK!=2.0.* in /usr/local/lib/python3.7/dist-packages (from torchio) (2.1.1)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from torchio) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchio) (4.62.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.7/dist-packages (from torchio) (1.2.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1->torchio) (3.10.0.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from Deprecated->torchio) (1.13.3)\n",
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzE7_7waF7_S"
      },
      "source": [
        "## Define arrays of patient and outcome data\n",
        "This block allows you to specify the criteria which defines the patient outcome as True or False. It then loops through all the patients in the metadata.csv file, searches for their corresponding image in the image folder, and then adds patient and outcome to either the training, testing, or validation array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KFIqmcw83Cl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8277b8e4-da40-41aa-ef1e-65f9786e8a76"
      },
      "source": [
        "# Open the metadata.csv file, convert to an array, and remove column headers\n",
        "metadata_file = open(project_folder + \"metadata.csv\")\n",
        "metadata = np.loadtxt(metadata_file, dtype=\"str\", delimiter=\",\")\n",
        "metadata = metadata[1:10][:]\n",
        "\n",
        "# Set the values which are used to define the outcome for each patient\n",
        "outcome_type = 1 #int(input(\"Select which outcome you are aiming to predict \\n(1=Locoregional, 2=Distant Metastasis, 3=Death):\"))\n",
        "check_day = 3000 #int(input(\"Select the number of days at which to check for event:\"))\n",
        "which_patients = 1 #int(input(\"Do you want to include patients whose last follow up is before the check day? (no = 0, yes = 1):\"))\n",
        "\n",
        "# Create empty arrays to store patient names and outcomes in\n",
        "patient_with_event = []\n",
        "patient_no_event = []\n",
        "outcomes_train = []\n",
        "outcomes_test = []\n",
        "images = []\n",
        "\n",
        "# Loop through each patient and identify whether they are true or false for the specified outcome from above\n",
        "for patient in metadata:\n",
        "  if (patient[(5+outcome_type)] == \"\") and (int(patient[5]) >= check_day):\n",
        "    # Last follow up after check day, no event\n",
        "    outcome = 0\n",
        "  elif (patient[(5+outcome_type)] == \"\") and (int(patient[5]) < check_day) and (which_patients == 0):\n",
        "    # Last follow up before check day, event unknown\n",
        "    continue\n",
        "  elif (patient[(5+outcome_type)] == \"\") and (int(patient[5]) < check_day) and (which_patients == 1):\n",
        "    outcome = 0\n",
        "  elif int(patient[(5+outcome_type)]) <= check_day:\n",
        "    # Event occurred before or on check day\n",
        "    outcome = 1\n",
        "  else:\n",
        "    # Event occurred after check day\n",
        "    outcome = 0\n",
        "  # No Image file found for patient\n",
        "  if not os.path.exists(project_folder + \"crop/Images/\" + patient[0] + \".nii\"):\n",
        "    print(\"No image found for patient \" + patient[0])\n",
        "    continue\n",
        "  \n",
        "  if outcome == 1:\n",
        "    patient_with_event.append([patient[0], outcome])\n",
        "  else:\n",
        "    patient_no_event.append([patient[0], outcome])\n",
        "\n",
        "# # Make arrays the same length\n",
        "# if len(patient_with_event) < len(patient_no_event):\n",
        "#   new_patient_no_event = random.sample(patient_no_event,len(patient_with_event))\n",
        "#   new_patient_with_event = patient_with_event\n",
        "# elif len(patient_with_event) > len(patient_no_event):\n",
        "#   new_patient_with_event = random.sample(patient_with_event, len(patient_no_event))\n",
        "#   new_patient_no_event = patient_no_event\n",
        "# elif len(patient_with_event) == len(patient_no_event):\n",
        "new_patient_no_event = patient_no_event\n",
        "new_patient_with_event = patient_with_event\n",
        "pos_weights = len(new_patient_no_event)/len(new_patient_with_event)\n",
        "# Add patient name, outcome and image to array\n",
        "seventy_percent_event = int(0.7*len(new_patient_with_event))\n",
        "seventy_percent_no_event = int(0.7*len(new_patient_no_event))\n",
        "\n",
        "print('NO event')\n",
        "print(len(new_patient_no_event))\n",
        "print('WITH event')\n",
        "print(len(new_patient_with_event))\n",
        "train_patients_event = random.sample(new_patient_with_event, seventy_percent_event)\n",
        "train_patients_no_event = random.sample(new_patient_no_event, seventy_percent_no_event)\n",
        "\n",
        "def remove(small_array, original_array):\n",
        "  for i in small_array:\n",
        "    original_array.remove(i)\n",
        "    \n",
        "  return original_array\n",
        "\n",
        "new_patients_with_event = remove(train_patients_event, new_patient_with_event)\n",
        "new_patient_no_event = remove(train_patients_no_event, new_patient_no_event)\n",
        "\n",
        "print('NO event')\n",
        "print(len(new_patient_no_event))\n",
        "print('WITH event')\n",
        "print(len(new_patient_with_event))\n",
        "\n",
        "\n",
        "fifty_percent_event = int(0.5*len(new_patient_with_event))\n",
        "fifty_percent_no_event = int(0.5*len(new_patient_no_event))\n",
        "\n",
        "validate_patients_event = random.sample(new_patient_with_event, fifty_percent_event)\n",
        "validate_patients_no_event = random.sample(new_patient_no_event, fifty_percent_no_event)\n",
        "\n",
        "new_patient_with_event = remove(validate_patients_event, new_patient_with_event)\n",
        "new_patient_no_event = remove(validate_patients_no_event, new_patient_no_event)\n",
        "\n",
        "print('NO event')\n",
        "print(len(new_patient_no_event))\n",
        "print('WITH event')\n",
        "print(len(new_patient_with_event))\n",
        "\n",
        "test_patients_event = new_patient_with_event\n",
        "test_patients_no_event = new_patient_no_event\n",
        "\n",
        "outcomes_train = train_patients_event + train_patients_no_event\n",
        "outcomes_validate = validate_patients_event + validate_patients_no_event\n",
        "outcomes_test = test_patients_event + test_patients_no_event\n",
        "\n",
        "print(outcomes_train)\n",
        "print(outcomes_validate)\n",
        "print(outcomes_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No image found for patient HN-CHUM-005\n",
            "NO event\n",
            "7\n",
            "WITH event\n",
            "1\n",
            "NO event\n",
            "3\n",
            "WITH event\n",
            "1\n",
            "NO event\n",
            "2\n",
            "WITH event\n",
            "1\n",
            "[['HN-CHUM-008', 0], ['HN-CHUM-007', 0], ['HN-CHUM-006', 0], ['HN-CHUM-003', 0]]\n",
            "[['HN-CHUM-001', 0]]\n",
            "[['HN-CHUM-002', 1], ['HN-CHUM-004', 0], ['HN-CHUM-009', 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfGjowelNEoF"
      },
      "source": [
        "## Define dataset class\n",
        "This block defines the class on which to build dataset objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjrRiSmq4mBF"
      },
      "source": [
        "# class Normalize(Dataset):\n",
        "#     def __init__(self):\n",
        "#       pass\n",
        "#     def __call__(self, vol):\n",
        "#         vol = (vol-vol.mean())/vol.std()\n",
        "#         return(vol) \n",
        "\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        #Normalize()\n",
        "    ])\n",
        "\n",
        "#window and levelling and this does normalise as well\n",
        "def windowLevelNormalize(image, level, window):\n",
        "    minval = level - window/2\n",
        "    maxval = level + window/2\n",
        "    wld = np.clip(image, minval, maxval)\n",
        "    wld -= minval\n",
        "    wld *= (1 / window)\n",
        "    return wld\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, annotations, img_dir, transform= data_transform, target_transform=None, rotate_augment=False, scale_augment=False, flip_augment=False, shift_augment=False):\n",
        "        self.img_labels = annotations\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.flips = flip_augment\n",
        "        self.rotations = rotate_augment\n",
        "        self.scaling = scale_augment\n",
        "        self.shifts = shift_augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels[idx][0]+\".nii\")\n",
        "        image_sitk = sitk.ReadImage(img_path)\n",
        "        image = sitk.GetArrayFromImage(image_sitk)\n",
        "        label = self.img_labels[idx][1]\n",
        "        print(image.shape)\n",
        "        \n",
        "        if self.shifts and random.random()<0.5:\n",
        "            mx_x, mx_yz = 10, 10\n",
        "            # find shift values\n",
        "            cc_shift, ap_shift, lr_shift = random.randint(-mx_x,mx_x), random.randint(-mx_yz,mx_yz), random.randint(-mx_yz,mx_yz)\n",
        "            # pad for shifting into\n",
        "            image = np.pad(image, pad_width=((mx_x,mx_x),(mx_yz,mx_yz),(mx_yz,mx_yz)), mode='constant', constant_values=-1024)\n",
        "            # crop to complete shift\n",
        "            image = image[mx_x+cc_shift:246+mx_x+cc_shift, mx_yz+ap_shift:246+mx_yz+ap_shift, mx_yz+lr_shift:246+mx_yz+lr_shift]\n",
        "            #print(image.shape)\n",
        "            #print('shift')\n",
        "\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        \n",
        "        if self.rotations and random.random()<0.5:\n",
        "            # taking implementation from my 3DSegmentationNetwork which can be applied -> rotations in the axial plane only I should think? -10->10 degrees?\n",
        "            roll_angle = np.clip(np.random.normal(loc=0,scale=3), -10, 10) # make -10,10\n",
        "            image = self.rotation(image, roll_angle, rotation_plane=(1,2)) # (1,2) originally\n",
        "            #print('rotation')\n",
        "            \n",
        "        if self.scaling and random.random()<0.5:\n",
        "            # same here -> zoom between 80-120%\n",
        "            scale_factor = np.clip(np.random.normal(loc=1.0,scale=0.5), 0.8, 1.2) # original scale = 0.05\n",
        "            image = self.scale(image, scale_factor)\n",
        "            #print('scale')\n",
        "            \n",
        "        if self.flips and random.random()<0.5:\n",
        "            image = self.flip(image)\n",
        "            #print('horizontal flip')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # window and levelling\n",
        "        image = windowLevelNormalize(image, level=40, window=80)\n",
        " \n",
        "        return image, label\n",
        "    def scale(self, image, scale_factor):\n",
        "        # scale the image or mask using scipy zoom function\n",
        "        order, cval = (3, 0) # changed from -1024 to 0\n",
        "        height, width, depth = image.shape\n",
        "        zheight = int(np.round(scale_factor*height))\n",
        "        zwidth = int(np.round(scale_factor*width))\n",
        "        zdepth = int(np.round(scale_factor*depth))\n",
        "        # zoomed out\n",
        "        if scale_factor < 1.0:\n",
        "            new_image = np.full_like(image, cval)\n",
        "            ud_buffer = (height-zheight) // 2\n",
        "            ap_buffer = (width-zwidth) // 2\n",
        "            lr_buffer = (depth-zdepth) // 2\n",
        "            new_image[ud_buffer:ud_buffer+zheight, ap_buffer:ap_buffer+zwidth, lr_buffer:lr_buffer+zdepth] = zoom(input=image, zoom=scale_factor, order=order, mode='constant', cval=cval)[0:zheight, 0:zwidth, 0:zdepth]\n",
        "            return new_image\n",
        "        elif scale_factor > 1.0:\n",
        "            new_image = zoom(input=image, zoom=scale_factor, order=order, mode='constant', cval=cval)[0:zheight, 0:zwidth, 0:zdepth]\n",
        "            ud_extra = (new_image.shape[0] - height) // 2\n",
        "            ap_extra = (new_image.shape[1] - width) // 2\n",
        "            lr_extra = (new_image.shape[2] - depth) // 2\n",
        "            new_image = new_image[ud_extra:ud_extra+height, ap_extra:ap_extra+width, lr_extra:lr_extra+depth]\n",
        "            return new_image\n",
        "        return image\n",
        "      \n",
        "    def rotation(self, image, rotation_angle, rotation_plane):\n",
        "        # rotate the image using scipy rotate function\n",
        "        order, cval = (3, -1024) # changed from -1024 to 0\n",
        "        return rotate(input=image, angle=rotation_angle, axes=rotation_plane, reshape=False, order=order, mode='constant', cval=cval)\n",
        "\n",
        "    def flip(self, image):\n",
        "        #hflip = np.fliplr(image)\n",
        "        #image = (reversed(image[1:]))\n",
        "        image = np.flipud(image).copy()\n",
        "        return image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiSjndmcNfSA"
      },
      "source": [
        "## Build Datasets\n",
        "This block uses the class and arrays defined previously to build datasets for training, testing and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecDoF-cH6xw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00dc319d-b606-4e2a-9242-59920cec6305"
      },
      "source": [
        "\n",
        "training_data = ImageDataset(outcomes_train, project_folder + \"crop/Images/\")\n",
        "validation_data = ImageDataset(outcomes_validate, project_folder + \"crop/Images/\", rotate_augment=False, scale_augment=False, flip_augment=False, shift_augment=False)\n",
        "test_data = ImageDataset(outcomes_test, project_folder + \"crop/Images/\", rotate_augment=False, scale_augment=False, flip_augment=False, shift_augment=False)\n",
        "print(len(training_data))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7jDjfvKOCWc"
      },
      "source": [
        "## View binary masks in 3d\n",
        "This block allows you to view a binary mask from the image in 3d by extracting the image from a given dataset. This helps to confirm that the data has not been affected by reading in to pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uideXDzjvdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd216a9-4d09-437d-f832-fe0fbcb5a293"
      },
      "source": [
        "  # Set which dataset to look at, and the index of the patient to view\n",
        "dataset = training_data\n",
        "index = 1\n",
        "print('flipud')\n",
        "print(outcomes_train[index])\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "#print(dataset[0])\n",
        "\n",
        "#array = dataset[index][0].numpy()\n",
        "array = dataset[index][0]\n",
        "print(type(array))\n",
        "print('array shape')\n",
        "print(array.shape)\n",
        "x,y,z = np.where(array > 0.) # what >=\n",
        "ax.scatter(x, y, z, c=z, alpha=1)\n",
        "\n",
        "ax.set_xlim(0,246)\n",
        "ax.set_ylim(0,246)\n",
        "ax.set_zlim(0,246)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flipud\n",
            "['HN-CHUM-007', 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(246, 246, 246)\n",
            "<class 'torch.Tensor'>\n",
            "array shape\n",
            "torch.Size([246, 246, 246])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 246.0)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2Chh969QU-u"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEBaWTtLN_A_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd003faf-fef2-454e-9b01-ebe25f57f475"
      },
      "source": [
        "train_dataloader = DataLoader(training_data, batch_size=4, shuffle=True)\n",
        "validate_dataloader = DataLoader(validation_data, batch_size=4, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=4, shuffle=True)\n",
        "\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF4hpRnHALqS"
      },
      "source": [
        "# Define CNN Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSj3U3DL5NGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d356c0-d939-444e-f261-9fe52a3b232f"
      },
      "source": [
        "# class CNN(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(CNN, self).__init__()\n",
        "#     out1 = 4\n",
        "#     out2 = 4\n",
        "#     out3 = 2\n",
        "#     self.cnn_layers = nn.Sequential(\n",
        "#       # Layer 1\n",
        "#       nn.Conv3d(1,out1,4,1,1),\n",
        "#       nn.BatchNorm3d(out1),\n",
        "#       #nn.ReLU(inplace=True),\n",
        "#       nn.LeakyReLU(inplace=True),\n",
        "#       nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "#       # Layer 2\n",
        "#       nn.Conv3d(out1, out2, 4, 1, 1),\n",
        "#       nn.BatchNorm3d(out2),\n",
        "#       #nn.ReLU(inplace=True),\n",
        "#       nn.LeakyReLU(inplace=True),\n",
        "#       nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "#       # Layer 3\n",
        "#       nn.Conv3d(out2, out3, 4, 1, 1),\n",
        "#       nn.BatchNorm3d(out3),\n",
        "#       #nn.ReLU(inplace=True),\n",
        "#       nn.LeakyReLU(inplace=True),\n",
        "#       nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "#     )\n",
        "    # self.linear_layers = nn.Sequential(\n",
        "    #   nn.Linear(48778, 2)\n",
        "    # )\n",
        "#   def forward(self, x):\n",
        "#     x = self.cnn_layers(x)\n",
        "#     x = x.view(x.size(0), -1)\n",
        "#     x = self.linear_layers(x)\n",
        "#     return x\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    out1 = 32\n",
        "    out2 = 64\n",
        "    out3 = 128\n",
        "    out4 = 64\n",
        "    out5 = 16\n",
        "    out6 = 2\n",
        "    self.cnn_layers = nn.Sequential(\n",
        "      # Layer 1\n",
        "      nn.Conv3d(1,out1,2,2),\n",
        "      nn.BatchNorm3d(out1),\n",
        "      nn.LeakyReLU(inplace=True),\n",
        "      nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "      # Layer 2\n",
        "      nn.Conv3d(out1, out2, 2, 2),\n",
        "      nn.BatchNorm3d(out2),\n",
        "      nn.LeakyReLU(inplace=True),\n",
        "      nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "      # Layer 3\n",
        "      nn.Conv3d(out2, out3, 2, 2),\n",
        "      nn.BatchNorm3d(out3),\n",
        "      nn.LeakyReLU(inplace=True),\n",
        "      nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "      # Layer 4\n",
        "      nn.Conv3d(out3, out4, 1, 1),\n",
        "      nn.BatchNorm3d(out4),\n",
        "      nn.LeakyReLU(inplace=True),\n",
        "      # Layer 5\n",
        "      nn.Conv3d(out4, out5, 1, 1),\n",
        "      nn.BatchNorm3d(out5),\n",
        "      nn.LeakyReLU(inplace=True),\n",
        "      # Layer 6\n",
        "      nn.Conv3d(out5, out6, 1, 1),\n",
        "      nn.BatchNorm3d(out6),\n",
        "      nn.LeakyReLU(inplace=True),\n",
        "      nn.AvgPool3d(2)\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.cnn_layers(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    return x\n",
        "\n",
        "\n",
        "model = CNN().to(device)\n",
        "print(model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (cnn_layers): Sequential(\n",
            "    (0): Conv3d(1, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "    (3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv3d(32, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (5): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "    (7): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
            "    (9): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "    (11): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "    (13): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "    (15): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "    (16): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (17): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "    (18): Conv3d(16, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "    (19): BatchNorm3d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (20): LeakyReLU(negative_slope=0.01, inplace=True)\n",
            "    (21): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imIsJYkHAVEe"
      },
      "source": [
        "# Define Train and Test Loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RhWjamUGtE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee88a402-0225-44f3-f5ee-48e6342a565f"
      },
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        X = reshape(X, (X.shape[0],1,246,246,246))\n",
        "        X = X.float()\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        print('y issssssssssssssss')\n",
        "\n",
        "\n",
        "        #y = reshape(y, (y.shape[0],1))\n",
        "        hot_y = torch.empty((X.shape[0],2)).to(device)\n",
        "        for index in range(len(y)):\n",
        "          if y[index] == 0:\n",
        "            hot_y[index,0] = 1\n",
        "            hot_y[index,1] = 0\n",
        "          elif y[index] == 1:\n",
        "            hot_y[index,0] = 0\n",
        "            hot_y[index,1] = 1\n",
        "      \n",
        "        print(hot_y)\n",
        "        pred = model(X)\n",
        "        torch.squeeze(pred)\n",
        "        loss = loss_fn(pred, hot_y.float())\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print results after each batch        \n",
        "        if batch % 1 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss\n",
        "\n",
        "def validate_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    validate_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = reshape(X, (X.shape[0],1,246,246,246))\n",
        "            X = X.float()\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            #y = reshape(y, (y.shape[0],1))\n",
        "            hot_y = torch.empty((X.shape[0],2)).to(device)\n",
        "            for index in range(len(y)):\n",
        "              if y[index] == 0:\n",
        "                hot_y[index,0] = 1\n",
        "                hot_y[index,1] = 0\n",
        "              elif y[index] == 1:\n",
        "                hot_y[index,0] = 0\n",
        "                hot_y[index,1] = 1\n",
        "            \n",
        "            pred = model(X)\n",
        "            # print(f'pred: {pred}')\n",
        "            # print(f'hot_y: {hot_y}')\n",
        "            _,predictions = torch.max(pred , 1)\n",
        "            _,targets = torch.max(hot_y, 1)\n",
        "            # print(f'predictions: {predictions}')\n",
        "            # print(f'targets: {targets}')\n",
        "            print(f'Correct this batch = {(predictions == targets).sum().item()}')\n",
        "\n",
        "            torch.squeeze(pred)\n",
        "            validate_loss += loss_fn(pred, hot_y.float()).item()\n",
        "            # correct += (pred.argmax(1) == hot_y).type(torch.float).sum().item()\n",
        "            correct += (predictions == targets).sum().item()\n",
        "\n",
        "    validate_loss /= num_batches\n",
        "    correct /= size\n",
        "    accuracy = 100*correct\n",
        "    print(f\"Validate Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {validate_loss:>8f} \\n\")\n",
        "    return validate_loss, accuracy\n",
        "\n",
        "learning_rate = 0.00101\n",
        "# defining the model\n",
        "model = CNN()\n",
        "# defining the optimizer\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# defining the loss function\n",
        "pos_weights = torch.tensor([(1/pos_weights), pos_weights])\n",
        "# print(pos_weights.size())\n",
        "# loss_fn = nn.BCEWithLogitsLoss()\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weights)\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "model.to(device)\n",
        "loss_fn.to(device)\n",
        "\n",
        "summary(model=model, input_size=(1, 246, 246, 246), batch_size=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv3d-1     [2, 32, 123, 123, 123]             288\n",
            "       BatchNorm3d-2     [2, 32, 123, 123, 123]              64\n",
            "         LeakyReLU-3     [2, 32, 123, 123, 123]               0\n",
            "         MaxPool3d-4        [2, 32, 61, 61, 61]               0\n",
            "            Conv3d-5        [2, 64, 30, 30, 30]          16,448\n",
            "       BatchNorm3d-6        [2, 64, 30, 30, 30]             128\n",
            "         LeakyReLU-7        [2, 64, 30, 30, 30]               0\n",
            "         MaxPool3d-8        [2, 64, 15, 15, 15]               0\n",
            "            Conv3d-9          [2, 128, 7, 7, 7]          65,664\n",
            "      BatchNorm3d-10          [2, 128, 7, 7, 7]             256\n",
            "        LeakyReLU-11          [2, 128, 7, 7, 7]               0\n",
            "        MaxPool3d-12          [2, 128, 3, 3, 3]               0\n",
            "           Conv3d-13           [2, 64, 3, 3, 3]           8,256\n",
            "      BatchNorm3d-14           [2, 64, 3, 3, 3]             128\n",
            "        LeakyReLU-15           [2, 64, 3, 3, 3]               0\n",
            "           Conv3d-16           [2, 16, 3, 3, 3]           1,040\n",
            "      BatchNorm3d-17           [2, 16, 3, 3, 3]              32\n",
            "        LeakyReLU-18           [2, 16, 3, 3, 3]               0\n",
            "           Conv3d-19            [2, 2, 3, 3, 3]              34\n",
            "      BatchNorm3d-20            [2, 2, 3, 3, 3]               4\n",
            "        LeakyReLU-21            [2, 2, 3, 3, 3]               0\n",
            "        AvgPool3d-22            [2, 2, 1, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 92,342\n",
            "Trainable params: 92,342\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 113.58\n",
            "Forward/backward pass size (MB): 2921.27\n",
            "Params size (MB): 0.35\n",
            "Estimated Total Size (MB): 3035.20\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w-_C54rIh_Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=content/drive/logsdir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfKkG2-Gh_p1",
        "outputId": "5aae2cd8-a2a1-43b5-83e3-1e15b7ec2986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.8.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "Error in atexit._run_exitfuncs:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/util.py\", line 320, in _exit_function\n",
            "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSqQYmcaAe-Q"
      },
      "source": [
        "# Run Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F23uXlNlG9ZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3be41f7-6a78-4035-ab8e-6e2624b873a1"
      },
      "source": [
        "epochs = 2\n",
        "train_losses = [[],[]]\n",
        "validate_losses = [[],[]]\n",
        "validate_accuracies = [[],[]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Custom tensorboard writer class\n",
        "\"\"\"\n",
        "\n",
        "class customWriter(SummaryWriter):\n",
        "    def __init__(self, log_dir, batch_size, epoch, num_classes, dataloader):\n",
        "        super(customWriter, self).__init__()\n",
        "        self.log_dir = log_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.epoch = epoch\n",
        "        self.num_classes = num_classes\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "        self.class_loss = {n: [] for n in range(num_classes+1)}\n",
        "        self.dataloader = dataloader\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1/(1+torch.exp(-x))\n",
        "\n",
        "    def reset_losses(self):\n",
        "        self.train_loss, self.val_loss, self.class_loss = [], [], {\n",
        "            n: [] for n in range(self.num_classes+1)}\n",
        "\n",
        "    def plot_batch(self, tag, images):\n",
        "        \"\"\"\n",
        "        Plot batches in grid\n",
        "â€‹\n",
        "        Args: tag = identifier for plot (string)\n",
        "              images = input batch (torch.tensor)\n",
        "        \"\"\"\n",
        "        img_grid = torchvision.utils.make_grid(images, nrow=self.batch_size // 2)\n",
        "        self.add_image(tag, img_grid)\n",
        "\n",
        "    # def plot_prediction(self, tag, prediction, target, plot_target=True):\n",
        "    #     \"\"\"\n",
        "    #     Plot predictions vs target segmentation.\n",
        "    #     Args: tag = identifier for plot (string)\n",
        "    #           prediction = batch output of trained model (torch.tensor)\n",
        "    #           target = batch ground-truth segmentations (torch.tensor)\n",
        "    #     \"\"\"\n",
        "    #     fig = plt.figure(figsize=(24, 24, 24))#changed from (24,24)\n",
        "    #     prediction = self.sigmoid(prediction)\n",
        "    #     for idx in np.arange(self.batch_size):\n",
        "    #         ax = fig.add_subplot(self.batch_size // 2, self.batch_size // 2, self.batch_size // 2,\n",
        "    #                             idx+1, label='segmentations')\n",
        "    #         ax.imshow(prediction[idx, 0].cpu().numpy(\n",
        "    #         ), cmap='viridis')\n",
        "    #         if plot_target:\n",
        "    #             ax.imshow(target[idx, 0].cpu().numpy(), cmap='gray', alpha=0.25)\n",
        "    #         ax.set_title('prediction @ epoch: {} - idx: {}'.format(self.epoch, idx))\n",
        "    #     self.add_figure(tag, fig)\n",
        "    def plot_pred(self, tag, prediction):\n",
        "        \"\"\"\n",
        "        Plot predictions vs target segmentation.\n",
        "        Args: tag = identifier for plot (string)\n",
        "              prediction = batch output of trained model (torch.tensor)\n",
        "              target = batch ground-truth segmentations (torch.tensor)\n",
        "        \"\"\"\n",
        "        fig = plt.figure(figsize=(24, 24))#changed from (24,24)\n",
        "        for idx in np.arange(self.batch_size):\n",
        "            ax = fig.add_subplot(self.batch_size // 2, self.batch_size // 2, self.batch_size // 2,\n",
        "                                idx+1, label='images')\n",
        "            ax.imshow(prediction[idx, 0].cpu().numpy(\n",
        "            ), cmap='viridis')\n",
        "            \n",
        "            ax.set_title('prediction @ epoch: {} - idx: {}'.format(self.epoch, idx))\n",
        "        self.add_figure(tag, fig)\n",
        "\n",
        "    def plot_tumour(self, tag, dataloader):\n",
        "        fig = plt.figure(figsize=(24, 24))\n",
        "        size = len(dataloader.dataset)\n",
        "        \n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X = reshape(X, (X.shape[0],1,246,246,246))\n",
        "            X = X.float()\n",
        "            print(X)\n",
        "            X = X.to(device)\n",
        "            #X.numpy()\n",
        "            X = X.cpu()\n",
        "            X = X.detach().numpy()\n",
        "            for i in range(X.shape[0]):\n",
        "                Xbig = X[i,0,:,:,:]\n",
        "                print(Xbig.shape)\n",
        "                print(type(Xbig))\n",
        "                Xsmall = Xbig[:,:,123]\n",
        "                print(Xsmall.shape)\n",
        "                print(type(Xsmall))\n",
        "                ax = fig.add_subplot()\n",
        "                ax.imshow(Xsmall, cmap='viridis')\n",
        "                self.add_figure(str(tag), fig)\n",
        "                tag += 1\n",
        "\n",
        "    def plot_histogram(self, tag, prediction):\n",
        "        print('Plotting histogram')\n",
        "        fig = plt.figure(figsize=(24, 24))\n",
        "        for idx in np.arange(self.batch_size):\n",
        "            ax = fig.add_subplot(self.batch_size // 2, self.batch_size // 2,\n",
        "                                 idx+1, yticks=[], label='histogram')\n",
        "            pred_norm = (prediction[idx, 0]-prediction[idx, 0].min())/(\n",
        "                prediction[idx, 0].max()-prediction[idx, 0].min())\n",
        "            ax.hist(pred_norm.cpu().flatten(), bins=100)\n",
        "            ax.set_title(\n",
        "                f'Prediction histogram @ epoch: {self.epoch} - idx: {idx}')\n",
        "        self.add_figure(tag, fig)\n",
        "\n",
        "    def per_class_loss(self, prediction, target, criterion, alpha=None):\n",
        "        # Predict shape: (4, 1, 512, 512)\n",
        "        # Target shape: (4, 1, 512, 512)\n",
        "        #pred, target = prediction.cpu().numpy(), target.cpu().numpy()\n",
        "        pred, target = prediction, target\n",
        "        for class_ in range(self.num_classes + 1):\n",
        "            class_pred, class_tgt = torch.where(\n",
        "                target == class_, pred, torch.tensor([0], dtype=torch.float32).cuda()),  torch.where(target == class_, target, torch.tensor([0], dtype=torch.float32).cuda())\n",
        "\n",
        "            #class_pred, class_tgt = pred[target == class_], target[target == class_] \n",
        "            if alpha is not None:\n",
        "                loss = criterion(class_pred, class_tgt, alpha)\n",
        "                #bce_loss, dice_loss = criterion(class_pred, class_tgt, alpha)\n",
        "            else:\n",
        "                loss = criterion(class_pred, class_tgt)\n",
        "                #bce_loss, dice_loss = criterion(class_pred, class_tgt)\n",
        "            #loss = bce_loss + dice_loss\n",
        "            self.class_loss[class_].append(loss.item())\n",
        "\n",
        "    def write_class_loss(self):\n",
        "        for class_ in range(self.num_classes+1):\n",
        "            self.add_scalar(f'Per Class loss for class {class_}', np.mean(self.class_loss[class_]), self.epoch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "writer = customWriter(project_folder, 2, 0, 1, train_dataloader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    validate_loss = validate_loop(validate_dataloader, model, loss_fn)\n",
        "\n",
        "    train_losses[0].append(t)\n",
        "    train_losses[1].append(train_loss)\n",
        "    validate_losses[0].append(t)\n",
        "    validate_losses[1].append(validate_loss[0])\n",
        "    validate_accuracies[0].append(t)\n",
        "    validate_accuracies[1].append(validate_loss[1])\n",
        "\n",
        "    writer.add_scalar('Train Loss', train_loss, t)\n",
        "    writer.add_scalar('Validate Loss', validate_loss[0], t)\n",
        "    # plot 3d plots here\n",
        "    writer.plot_tumour(dataloader = train_dataloader, tag=tag)\n",
        "writer.close()\n",
        "print(\"Done!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "(246, 246, 246)\n",
            "(246, 246, 246)\n",
            "(246, 246, 246)\n",
            "(246, 246, 246)\n",
            "y issssssssssssssss\n",
            "tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0')\n",
            "loss: 2.452642  [    0/    4]\n",
            "(246, 246, 246)\n",
            "Correct this batch = 1\n",
            "Validate Error: \n",
            " Accuracy: 100.0%, Avg loss: 2.449444 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(246, 246, 246)\n",
            "(246, 246, 246)\n",
            "(246, 246, 246)\n",
            "(246, 246, 246)\n",
            "tensor([[[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]]]]])\n",
            "(246, 246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(246, 246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "(246, 246, 246)\n",
            "(246, 246, 246)\n",
            "(246, 246, 246)\n",
            "(246, 246, 246)\n",
            "y issssssssssssssss\n",
            "tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], device='cuda:0')\n",
            "loss: 2.451806  [    0/    4]\n",
            "(246, 246, 246)\n",
            "Correct this batch = 1\n",
            "Validate Error: \n",
            " Accuracy: 100.0%, Avg loss: 2.449681 \n",
            "\n",
            "(246, 246, 246)\n",
            "(246, 246, 246)\n",
            "(246, 246, 246)\n",
            "(246, 246, 246)\n",
            "tensor([[[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           ...,\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "           [0., 0., 0.,  ..., 0., 0., 0.]]]]])\n",
            "(246, 246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "(246, 246)\n",
            "<class 'numpy.ndarray'>\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "NUCtjrHb6JAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(dataloader, model):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = reshape(X, (X.shape[0],1,246,246,246))\n",
        "            X = X.float()\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            #y = reshape(y, (y.shape[0],1))\n",
        "            hot_y = torch.empty((X.shape[0],2)).to(device)\n",
        "            for index in range(len(y)):\n",
        "              if y[index] == 0:\n",
        "                hot_y[index,0] = 1\n",
        "                hot_y[index,1] = 0\n",
        "              elif y[index] == 1:\n",
        "                hot_y[index,0] = 0\n",
        "                hot_y[index,1] = 1\n",
        "                \n",
        "            pred = model(X)\n",
        "            _,predictions = torch.max(pred , 1)\n",
        "            _,targets = torch.max(hot_y, 1)\n",
        "            torch.squeeze(pred)\n",
        "            #test_loss += loss_fn(pred, y.float()).item()\n",
        "            test_loss += loss_fn(pred, hot_y.float()).item()\n",
        "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            correct += (predictions == targets).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n",
        "\n",
        "test_loop(test_dataloader, model)"
      ],
      "metadata": {
        "id": "MEuNq5Z-6IV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZBw098GbFou"
      },
      "source": [
        "# Plot Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsmncBB6bEip"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "ax.plot(train_losses[0], train_losses[1], label=\"Train Loss\")\n",
        "ax.plot(validate_losses[0], validate_losses[1], label=\"Validate Loss\")\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Accuracies"
      ],
      "metadata": {
        "id": "fFAdAFAECQ-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "ax.plot(validate_accuracies[0], validate_accuracies[1], label=\"Validate Accuracies\")\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Accuracy / %')\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "2Q8g4JowCPkC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}